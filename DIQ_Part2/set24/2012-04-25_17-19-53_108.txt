306
ieee transactions on robotics and automation, vol. 7. no. 3. june 1991
Segmentation via Manipulation
Constantine J. Tsikos and Ruzena K. Bajcsy, Senior Member, IEEE
Abstract—introduce the paradigm of iterative, interactive scene segmentation and simplification of random heaps of unknown objects via vision and manipulation. The scene simplification is based on the graph operations of vertex and edge removal. These operations are defined isomorphic to the pick and push manipulation actions. We use the sensors as graph generators and the manipulator as the decomposing mechanism of the graphs. The model is a nondeterministic finite-state Turing Machine. We integrated a vision system, a manipulator, and force/torque and other sensory Input into a robot work cell and conducted experiments to test convergence and error recovery of four different strategies. We found that under certain conditions the strategies can tolerate errors in the sensory data, recover from pathological states, and converge.
I. Introduction
THE motivation for this paper is the observation that a scene containing more than one object most of the time cannot be segmented only by vision or in general by any noncontact sensing method. Visual information may be sufficient to accurately segment simple objects and nonoverlap-ping scenes. However, in general, it is not sufficient for random heaps of unknown objects.
If no a priori knowledge is available, the vision system cannot reliably distinguish between overlaps caused by two different objects in the scene and overlaps caused by a single self-occluding object. A flat rigid object supported by and totally occluding another smaller object may be recognized as a large box-shaped object. Similarly, a flat nonrigid object supported in the middle by a smaller object may be recognized as convex, while if it is supported at the edges by more than one object, it may be recognized as concave.
Therefore, machine vision alone (or any noncontact sensing method) is not sufficient for segmentation and recognition. An exception to this may be the case when the objects are physically separated so that the noncontact sensor can measure this separation or one knows a priori a great deal about the objects (their geometry, material, etc.).
Manuscript received March 24, 1989; revised January 1.1990. This work was supported by the U.S Postal Service BOA under Contract 104230-87-H-0001/M-0195; by the U.S. Air Force under Grant AFOSR F49620-85-K-0018; by the U.S. Army under Grant DAAG-29-S4-K-0061; by the National Science Foundation under Grants CER/DCR82-19196 Ao2, INKS-14199, DMC&5-17315; by NASA under Grant NAG5-1045; by the ONR under Grant 38-35923-0; by the NIH under Grants NS-10939-11 (as part of the Cerebro Vascular Research Center) and 1-R01-NS-23636-01; by ARPA under Grant N0014-U-K-0632; by NATO under Grant 0024/85, and by the DEC Corporation. IBM, and the LORD Corporation.
C 1. Tsikos was with the Department of Computer and Information Science, University of Pennsylvania. Philadelphia. PA 19104-6389 He is now w«fc fee Siemens Corporate Research Laboratories, Princeton. NJ 01540.
R K- Bajcsy is with the Department of Computer and Information Science. University of Pennsylvania. Philadelphia, PA 19104-6389.
IfiEE Log Number 9142996
The traditional approach is to segment the noncontact sensory information (range, intensity, etc.) regardless of scene complexity. Then, based on the outcome of segmentation, to interpret the scene and recognize the objects. The problem with this approach is that reliability decreases when scenes become more complex and when a priori assumptions are removed.
Our approach is different. Instead of trying to deal with an ever increasing visual scene complexity, we use the manipulator to make the scene simpler for the vision system. Our paradigm is analogous to having the hand help the eye when interpretation of visual information is ambiguous, or when the scene is visually complex.
Our system is iterative because random arrangements of objects form layers. Due to our noncontact sensor arrangement, only the top layer of the heap is visible at any given time and the objects are removed from the scene one at a time. In general, the system must sense and manipulate more than once for every random scene.
The system is interactive because the vision system may request a manipulatory action to resolve an interpretation ambiguity, to reduce visual complexity, or to grasp and remove an object from the scene. The manipulatory action must be monitored by the noncontact sensor (vision system) as well as the contact sensors (force/torque) in a closed loop.
A.	Assumptions
Our assumptions are:
1)	The scene is reachable by the manipulator and accessible to the sensors, i.e., the entire scene is visible, although occlusions may occur.
2)	The scene is decomposable; it consists of convex objects held together by the forces of gravity and friction.
3)	The objects may or may not be rigid. Their size and weight is such that they are manipulable with a suitable end effector. The complexity of the scene is bounded, i.e., typical scenes are three to six layers deep and may contain 15 to 30 objects.
4)	There is a well defined goal state that is detectable by the available sensors. The goal may be an empty scene or an organized /ordered scene.
B.	Domain
The domain is the class of irregular parcels and pieces (IPP) found in a post office environment. The class consists of rigid and nonrigid flats, boxes, tubes, and rolls. The objects have different weights, sizes, colors, visual surface textures (address labels, stamps, and other markings), varying porocity, coefficients of friction, and rigidity. Because many of these objects are not rigid, their true geometric shape cannot be measured; it is rather a toctkm of wtoere the
1042 296X/9 ^0600^)30650! 00 C 1991 IEEE