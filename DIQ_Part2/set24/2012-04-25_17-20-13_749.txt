30ft
be mMucnoo on robotics and automaton, vol,. » x kjne m>
Segmentation via Manipulation
Constantine J. Tsikos and Ruzena K. Bajcsy. Senior Member, IEEE
Abstract—V/t iatrodice the paradigm of UtiiOw, interactive scene KgnnUtiot and ii«pMfc»doi of random heaps of it-kaowis objects via tUn and ■iiipiiitioi. The nai tfcnptHi cstkM b based on the graph operations of vertex and e^p null. These operations are defined isomorphic to the $kk and posh manipulation actions. We w the whii aa g»fl genu Ion and the manipulator as the decomposing mechanism of the graphs. The model'm a nondetemlnistk InUi ifii Tnr ing Machine. We integrated a vision system, a maaspaiaior, and force/turf i and other sensory Inpnt into a robot work eafl and coadacud eqMrimats to teat cwmr|ta(« and error recovery of fonr iiimal strategies. We fonad that under certain conitfMa the strategies cnn tolerate errors in the unaory data, w»w from pathological states, and converge.
L iKTftOOUCriON
THE motivation for this paper is the observation that • scene containing more than one object moat of the time cannot be segmented only by vision or m general by any noncontact sensing method. Visual information may be snf ficient to accurately segment simple objects and nonoverlapping scenes. However, in general, it is not sufficient for random heaps of unknown objects.
If no a priori knowledge is available, the vision system cannot reliably distinguish between overlaps caused by two different objects in the scene and overlaps caused by a single self-occluding object. A flat rigid object supported by and totally occluding another smaller object may be recogmzed aa a large box-shaped object. Similarly, a flat nonrigid object supported in the middle by a smaller object may be recognized as convex, while if it is supported at the edges by more than one object, it may be recognized as concave.
Therefore, machine vision alone (or any noncontact sensing method) is not sufficient for segmentation and recognition. An exception to this may be the case when the objects are physically separated so that the noncontact sensor can measure this separation or one knows a priori a great deal about the objects (their geometry, material, etc.).
Manuscript received March 24, 1989, revised January 1, 1990. This work was supported by the U.S. Postal Service BOA under Contract 104230-87-H-0001/M-0195; by the U.S. Air Force under Grant APOSR F49620-S5-K-0018; by the U.S. Army under Grant DAAG-29 S4-K-0061; by the National Scieace Foundation under Grants CER/DCR82-19196 Ao2, INTSS-14199, DMCS5-17315; by NASA under Grant NA05-1045; by the ONR under Grant SB-35923-0; by the NIH under Grantt NS-10939-11 (as part of the Cerebro Vascular Research Center) and 1-R01-NS-236364I; by ARPA under Grant N0014-8&-K-0632; by NATO under Orant 0024/85, and by the DEC Corporation. IBM, and the LORD Corporation
C. J. Tsikos was with the Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104-6389. He is now with the Siemens Corporate Research Laboratories, Princeton, NJ 08540.
R. K. Bajcsy is with the Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104-6389.
IEEE Log Number 9142996.
The traditional approach in to icgmcat the nomomart sensory information (range, intensity, etc.) regardless of scene coaopkxjty. Tbts. baaed on the mmcon of segmentation. in interpret the scene and tnnfmir the objects. The problem with das approach is An idiaMny decreases when scenes become more complex and «ks m priori — mnptioni are meowed.
Oar approach n different iastead of trymg to deal with aa ever increasing visual scene compleajiy. we nse the mampo-lator in make the scene simpler lor the vision system. Our paradigm is analogous m havmg the hand help the eye vhes interpretation of final mfomaixm is Mdnfam. or when the scene ta vinmRy uanpha.
Oar if mm a Iiemivc because rsnrtiii arrangements of objects form layers Doe to oar nit—t sensor arrange mot, only the lop layer of it heap a virible at any given time and Efce objects arc removed from the scene one at a time hi general, the ay nun meat sense and mnmpnlate more than once for every random scene
The system n amnsum tow aum the vision system may request a manipulatory action to resolve an interpretation ambiguity, to rcdnce rami complexity, or to grasp and remove an object from the scene. The manipulatory action must be monitored by the noncontact sensor (vision system) m well aa the contact sensors (force/torque) in a closed loop.
A.	Assumptions
Our assumptions are:
1)	The scene is reachable by die manipulator aad accessible to the sensors, i.e., the entire scene is visible, although occlusions may occur.
2)	The scene is decomposable; it consists of convex objects held together by the forces of gravity and friction.
3)	The objects may or may not be rigid. Their size and weight is such that they are manipulabk with a suitable end effector. The complexity of the scene is bounded, i.e., typical scenes are three to six layers deep and may contain 15 to 30 objects.
4)	There is s well defined goal state that is detectable by the available sensors. The goal may be an empty scene or an organized /ordered scene.
B.	Domain
The domain is the class of irregular parcels and pieces (IPP) found in a post office environment. The class consists of rigid and nonrigid flats, boxes, tubes, and rolls. The objects have different weights, sizes, colors, visual surface textures (address labels, stamps, and other markings), varying porocity, coefficients of friction, and rigidity. Because many of these objects are not rigid, their true geometric shape cannot be measured; it is rather a function of where the
1042-296X/91/0600-0306$01.00 © 1991 IEEE